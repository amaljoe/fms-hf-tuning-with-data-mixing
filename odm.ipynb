{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-13T16:24:55.890860Z",
     "start_time": "2025-03-13T16:24:55.882975Z"
    }
   },
   "source": [
    "import random\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "class SmoothedMeanWeightUpdater:\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset_names: List[str],\n",
    "            weights: List[float],\n",
    "            smoothing_factor: float = 0.9,\n",
    "    ):\n",
    "        self.dataset_names = dataset_names\n",
    "        self.dataset_map = {name: i for i, name in enumerate(dataset_names)}\n",
    "        self.num_datasets = len(dataset_names)\n",
    "        self.weights = weights\n",
    "        self._estimated_reward = {name: 0.0 for name in dataset_names}\n",
    "        total_weights = np.sum(weights)\n",
    "        self._probabilities = {name: weight/total_weights for name, weight in zip(dataset_names, weights)}\n",
    "        self.eps = 1/self.num_datasets\n",
    "        self.prev_eps = None\n",
    "        self.smoothing_factor = smoothing_factor\n",
    "        self.vars_to_log = [\"_probabilities\", \"_estimated_reward\"]\n",
    "\n",
    "    def update(self, dataset_name: str, reward: float, iteration: int) -> List[float]:\n",
    "        \"\"\"\n",
    "        Updates the weights based on the provided reward.\n",
    "        \"\"\"\n",
    "\n",
    "        # update cumulative estimated reward\n",
    "        self._estimated_reward[dataset_name] = self.smoothing_factor*self._estimated_reward[dataset_name] + (1-self.smoothing_factor)*math.exp(reward)\n",
    "\n",
    "        # calculate epsilons\n",
    "        self.prev_eps = self.eps\n",
    "        self.eps = min(1/self.num_datasets, math.sqrt(math.log(self.num_datasets)/(self.num_datasets*iteration)))\n",
    "\n",
    "        # calculate scaling factor\n",
    "        total_estimated_rewards = sum([math.exp(r*self.prev_eps) for r in self._estimated_reward.values()])\n",
    "        scaling_factor = (1-self.num_datasets*self.eps)/total_estimated_rewards\n",
    "\n",
    "        # update weights\n",
    "        for name in self.dataset_names:\n",
    "            self.weights[self.dataset_map[name]] = math.exp(self._estimated_reward[name]*self.prev_eps)*scaling_factor + self.eps\n",
    "\n",
    "        # update probabilities\n",
    "        total_weights = sum(self.weights)\n",
    "        for name in self.dataset_names:\n",
    "            self._probabilities[name] = self.weights[self.dataset_map[name]]/total_weights\n",
    "\n",
    "        return list(self._probabilities.values())\n",
    "\n",
    "    def group_update(self, dataset_names: List[str], rewards: List, iteration: int):\n",
    "        # calculate epsilons\n",
    "        self.prev_eps = self.eps\n",
    "        self.eps = min(1/self.num_datasets, math.sqrt(math.log(self.num_datasets)/(self.num_datasets*iteration)))\n",
    "\n",
    "        # update cumulative estimated reward\n",
    "        for name, reward in zip(dataset_names, rewards):\n",
    "            # smoothed mean\n",
    "            # self._estimated_reward[name] = self.smoothing_factor*self._estimated_reward[name] + (1-self.smoothing_factor)*reward\n",
    "            # smoothed exponentiated mean\n",
    "            self._estimated_reward[name] = self.smoothing_factor*self._estimated_reward[name] + (1-self.smoothing_factor)*math.exp(reward)\n",
    "        # print(f\"Rank: {torch.distributed.get_rank()} -- estimated_reward {self._estimated_reward}\")\n",
    "\n",
    "        # calculate normalized scaling factor\n",
    "        total_estimated_rewards = sum((r*self.prev_eps) for r in self._estimated_reward.values())\n",
    "        scaling_factor = (1-self.num_datasets*self.eps)/total_estimated_rewards\n",
    "\n",
    "        # update weights\n",
    "        for name in self.dataset_names:\n",
    "            # self.weights[self.dataset_map[name]] = math.exp(self._estimated_reward[name]*self.prev_eps)*scaling_factor + self.eps\n",
    "            self.weights[self.dataset_map[name]] = self._estimated_reward[name]*self.prev_eps*scaling_factor + self.eps\n",
    "\n",
    "        # update probabilities\n",
    "        total_weights = sum(self.weights)\n",
    "        for name in self.dataset_names:\n",
    "            self._probabilities[name] = self.weights[self.dataset_map[name]]/total_weights\n",
    "\n",
    "        return list(self._probabilities.values())"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:25:48.499482Z",
     "start_time": "2025-03-13T14:25:48.497313Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "999c117e693759dc",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:31:00.263318Z",
     "start_time": "2025-03-13T14:31:00.259296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "weights = SmoothedMeanWeightUpdater([\"a\", \"b\"], [0.5, 0.5])\n",
    "weights.update(\"a\", 4, 1)\n",
    "weights.update(\"b\", 4, 2)\n",
    "weights.update(\"a\", 2, 3)\n",
    "weights.update(\"a\", 0, 3)"
   ],
   "id": "eb303151b979bb20",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4925946534736098, 0.5074053465263902]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T16:06:00.265720Z",
     "start_time": "2025-03-13T16:05:55.779316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from numpy import dtype\n",
    "from pandas import DataFrame\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "model_path=\"base_models/granite-3.2-2b-instruct\"\n",
    "device= \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path\n",
    ")"
   ],
   "id": "dcc852072d46b42b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1af98faed72d458796ad7c877522550e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T16:06:01.209670Z",
     "start_time": "2025-03-13T16:06:00.837325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "with open(\"data/coco.ml.txt\") as f:\n",
    "    ml = f.readlines()\n",
    "\n",
    "with open(\"data/coco.en.txt\") as f:\n",
    "    eng = f.readlines()\n",
    "\n",
    "def get_dataset(ml, eng):\n",
    "    ml = [sen.strip() for sen in ml]\n",
    "    eng = [sen.strip() for sen in eng]\n",
    "    return [{\"ml\": ml, \"eng\": eng, \"content\": f'Translate to english:<|end_of_text|>{ml}<|end_of_text|>{eng}<|end_of_text|>'} for ml, eng in zip(ml, eng)]\n",
    "\n",
    "dataset = get_dataset(ml, eng)\n",
    "n = 100\n",
    "train_dataset = Dataset.from_list(dataset[:n // 10 * 8])\n",
    "valid_dataset = Dataset.from_list(dataset[n // 10 * 8:n])\n",
    "dataset = DatasetDict({\"train\": train_dataset, \"validation\": valid_dataset})\n",
    "dataset"
   ],
   "id": "a1d2c6cdc5597efc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ml', 'eng', 'content'],\n",
       "        num_rows: 80\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['ml', 'eng', 'content'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T16:06:05.206837Z",
     "start_time": "2025-03-13T16:06:05.112591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "args = TrainingArguments(\"ml_to_en\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"content\"], truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].clone()\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, remove_columns=dataset['train'].column_names, batched=True)\n",
    "tokenized_datasets"
   ],
   "id": "a6ad464e7809412d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d8c66ca82dc04c5495e5eb6c1fe489d2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bcff67ba843c4755b32463c049d11226"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 80\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T16:28:26.294571Z",
     "start_time": "2025-03-13T16:26:50.428091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm.auto import tqdm\n",
    "from transformers import get_scheduler, DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle=True, batch_size=1, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=1, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "data_loaders = {\n",
    "    \"train\": train_dataloader,\n",
    "    \"eval\": eval_dataloader,\n",
    "}\n",
    "\n",
    "data_loader_iters = {k: iter(v) for k, v in data_loaders.items()}\n",
    "weights = SmoothedMeanWeightUpdater([\"train\", \"eval\"], [0.5, 0.5])\n",
    "\n",
    "model.train()\n",
    "for i in range(100):\n",
    "    batch_name = random.choices([\"train\", \"eval\"], weights=weights.weights)[0]\n",
    "    print(batch_name)\n",
    "    try:\n",
    "        batch = next(data_loader_iters[batch_name]) \n",
    "    except StopIteration:\n",
    "        data_loader_iters[batch_name] = iter(data_loaders[batch_name])\n",
    "        batch = next(data_loader_iters[batch_name])\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    res = weights.update(batch_name, loss.item(), i + 1)\n",
    "    print(loss, res)\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "    progress_bar.update(1)"
   ],
   "id": "b06650651ce05471",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/240 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d8979182da654c7f87d39807110c42db"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "tensor(1.9081, device='mps:0', grad_fn=<NllLossBackward0>) [0.5, 0.5]\n",
      "train\n",
      "tensor(2.7949, device='mps:0', grad_fn=<NllLossBackward0>) [0.5425724962209038, 0.4574275037790962]\n",
      "train\n",
      "tensor(1.8533, device='mps:0', grad_fn=<NllLossBackward0>) [0.580491365300458, 0.419508634699542]\n",
      "train\n",
      "tensor(1.8066, device='mps:0', grad_fn=<NllLossBackward0>) [0.5966139573228493, 0.40338604267715084]\n",
      "eval\n",
      "tensor(1.6550, device='mps:0', grad_fn=<NllLossBackward0>) [0.5826562473732418, 0.4173437526267581]\n",
      "train\n",
      "tensor(1.2114, device='mps:0', grad_fn=<NllLossBackward0>) [0.5828781005643748, 0.4171218994356253]\n",
      "train\n",
      "tensor(1.7071, device='mps:0', grad_fn=<NllLossBackward0>) [0.5888062416321191, 0.41119375836788086]\n",
      "eval\n",
      "tensor(1.4757, device='mps:0', grad_fn=<NllLossBackward0>) [0.575374294183836, 0.42462570581616393]\n",
      "train\n",
      "tensor(1.7778, device='mps:0', grad_fn=<NllLossBackward0>) [0.581371361754169, 0.418628638245831]\n",
      "eval\n",
      "tensor(1.2187, device='mps:0', grad_fn=<NllLossBackward0>) [0.5723022681507325, 0.42769773184926757]\n",
      "train\n",
      "tensor(1.0427, device='mps:0', grad_fn=<NllLossBackward0>) [0.5685796305104401, 0.4314203694895599]\n",
      "train\n",
      "tensor(1.2984, device='mps:0', grad_fn=<NllLossBackward0>) [0.5675393844140142, 0.43246061558598586]\n",
      "train\n",
      "tensor(1.0621, device='mps:0', grad_fn=<NllLossBackward0>) [0.5643895130053183, 0.4356104869946818]\n",
      "eval\n",
      "tensor(0.9386, device='mps:0', grad_fn=<NllLossBackward0>) [0.5592205283007746, 0.4407794716992254]\n",
      "eval\n",
      "tensor(1.6129, device='mps:0', grad_fn=<NllLossBackward0>) [0.5480460848400623, 0.4519539151599376]\n",
      "eval\n",
      "tensor(1.2547, device='mps:0', grad_fn=<NllLossBackward0>) [0.5422289053433591, 0.4577710946566409]\n",
      "train\n",
      "tensor(1.1452, device='mps:0', grad_fn=<NllLossBackward0>) [0.5406542499469318, 0.4593457500530682]\n",
      "eval\n",
      "tensor(1.0348, device='mps:0', grad_fn=<NllLossBackward0>) [0.5374406333394135, 0.4625593666605866]\n",
      "eval\n",
      "tensor(1.9236, device='mps:0', grad_fn=<NllLossBackward0>) [0.524455198652999, 0.47554480134700094]\n",
      "train\n",
      "tensor(0.7502, device='mps:0', grad_fn=<NllLossBackward0>) [0.5208364997005525, 0.4791635002994476]\n",
      "eval\n",
      "tensor(1.7433, device='mps:0', grad_fn=<NllLossBackward0>) [0.5124843797086863, 0.48751562029131373]\n",
      "train\n",
      "tensor(1.0403, device='mps:0', grad_fn=<NllLossBackward0>) [0.5112081386900361, 0.48879186130996394]\n",
      "train\n",
      "tensor(0.8981, device='mps:0', grad_fn=<NllLossBackward0>) [0.5091933121188914, 0.4908066878811087]\n",
      "train\n",
      "tensor(1.0697, device='mps:0', grad_fn=<NllLossBackward0>) [0.5084943500463479, 0.49150564995365204]\n",
      "eval\n",
      "tensor(0.9532, device='mps:0', grad_fn=<NllLossBackward0>) [0.5087631484354175, 0.4912368515645825]\n",
      "train\n",
      "tensor(0.8249, device='mps:0', grad_fn=<NllLossBackward0>) [0.5067172336491375, 0.4932827663508626]\n",
      "eval\n",
      "tensor(0.9594, device='mps:0', grad_fn=<NllLossBackward0>) [0.5069328918729563, 0.49306710812704374]\n",
      "train\n",
      "tensor(0.8757, device='mps:0', grad_fn=<NllLossBackward0>) [0.5054200473487721, 0.49457995265122795]\n",
      "eval\n",
      "tensor(0.9579, device='mps:0', grad_fn=<NllLossBackward0>) [0.5056280821234347, 0.49437191787656537]\n",
      "eval\n",
      "tensor(1.3198, device='mps:0', grad_fn=<NllLossBackward0>) [0.5033662080803593, 0.4966337919196408]\n",
      "eval\n",
      "tensor(0.8721, device='mps:0', grad_fn=<NllLossBackward0>) [0.5042395565286903, 0.4957604434713097]\n",
      "train\n",
      "tensor(1.0526, device='mps:0', grad_fn=<NllLossBackward0>) [0.5039460064689818, 0.4960539935310183]\n",
      "train\n",
      "tensor(0.8008, device='mps:0', grad_fn=<NllLossBackward0>) [0.5023647840703329, 0.4976352159296671]\n",
      "eval\n",
      "tensor(1.5349, device='mps:0', grad_fn=<NllLossBackward0>) [0.49853458708174336, 0.5014654129182566]\n",
      "train\n",
      "tensor(0.9216, device='mps:0', grad_fn=<NllLossBackward0>) [0.4977790576002376, 0.5022209423997624]\n",
      "eval\n",
      "tensor(1.6926, device='mps:0', grad_fn=<NllLossBackward0>) [0.49287162056827516, 0.507128379431725]\n",
      "eval\n",
      "tensor(1.4822, device='mps:0', grad_fn=<NllLossBackward0>) [0.49059637987740123, 0.5094036201225989]\n",
      "eval\n",
      "tensor(1.2791, device='mps:0', grad_fn=<NllLossBackward0>) [0.49018410501679555, 0.5098158949832046]\n",
      "eval\n",
      "tensor(1.0958, device='mps:0', grad_fn=<NllLossBackward0>) [0.49099488471126546, 0.5090051152887346]\n",
      "train\n",
      "tensor(0.6186, device='mps:0', grad_fn=<NllLossBackward0>) [0.48916713712198173, 0.5108328628780183]\n",
      "eval\n",
      "tensor(1.6095, device='mps:0', grad_fn=<NllLossBackward0>) [0.48608874169807403, 0.513911258301926]\n",
      "eval\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mStopIteration\u001B[0m                             Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 40\u001B[0m\n\u001B[1;32m     38\u001B[0m batch_name \u001B[38;5;241m=\u001B[39m random\u001B[38;5;241m.\u001B[39mchoices([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124meval\u001B[39m\u001B[38;5;124m\"\u001B[39m], weights\u001B[38;5;241m=\u001B[39mweights\u001B[38;5;241m.\u001B[39mweights)[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28mprint\u001B[39m(batch_name)\n\u001B[0;32m---> 40\u001B[0m batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(data_loader_iters[batch_name]) \n\u001B[1;32m     41\u001B[0m batch \u001B[38;5;241m=\u001B[39m {k: v\u001B[38;5;241m.\u001B[39mto(device) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m batch\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[1;32m     42\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mbatch)\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/llms-general/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    698\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    699\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    700\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 701\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_data()\n\u001B[1;32m    702\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    703\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    704\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[1;32m    705\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    706\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[1;32m    707\u001B[0m ):\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/llms-general/lib/python3.11/site-packages/torch/utils/data/dataloader.py:756\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    755\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 756\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    757\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_fetcher\u001B[38;5;241m.\u001B[39mfetch(index)  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    758\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/llms-general/lib/python3.11/site-packages/torch/utils/data/dataloader.py:691\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter._next_index\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    690\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_index\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 691\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter)\n",
      "\u001B[0;31mStopIteration\u001B[0m: "
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T16:21:20.180922Z",
     "start_time": "2025-03-13T16:21:20.178207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "arr = [1, 2]\n",
    "ite = iter(arr)\n",
    "\n",
    "next(ite), next(ite)\n",
    "\n",
    "# reset iterator\n",
    "ite = iter(arr)\n",
    "next(ite), next(ite)"
   ],
   "id": "d7246153089b64d9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
