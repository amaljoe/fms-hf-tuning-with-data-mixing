{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-13T16:24:55.890860Z",
     "start_time": "2025-03-13T16:24:55.882975Z"
    }
   },
   "source": [
    "import random\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "class SmoothedMeanWeightUpdater:\n",
    "    def __init__(\n",
    "            self,\n",
    "            dataset_names: List[str],\n",
    "            weights: List[float],\n",
    "            smoothing_factor: float = 0.9,\n",
    "    ):\n",
    "        self.dataset_names = dataset_names\n",
    "        self.dataset_map = {name: i for i, name in enumerate(dataset_names)}\n",
    "        self.num_datasets = len(dataset_names)\n",
    "        self.weights = weights\n",
    "        self._estimated_reward = {name: 0.0 for name in dataset_names}\n",
    "        total_weights = np.sum(weights)\n",
    "        self._probabilities = {name: weight/total_weights for name, weight in zip(dataset_names, weights)}\n",
    "        self.eps = 1/self.num_datasets\n",
    "        self.prev_eps = None\n",
    "        self.smoothing_factor = smoothing_factor\n",
    "        self.vars_to_log = [\"_probabilities\", \"_estimated_reward\"]\n",
    "\n",
    "    def update(self, dataset_name: str, reward: float, iteration: int) -> List[float]:\n",
    "        \"\"\"\n",
    "        Updates the weights based on the provided reward.\n",
    "        \"\"\"\n",
    "\n",
    "        # update cumulative estimated reward\n",
    "        self._estimated_reward[dataset_name] = self.smoothing_factor*self._estimated_reward[dataset_name] + (1-self.smoothing_factor)*math.exp(reward)\n",
    "\n",
    "        # calculate epsilons\n",
    "        self.prev_eps = self.eps\n",
    "        self.eps = min(1/self.num_datasets, math.sqrt(math.log(self.num_datasets)/(self.num_datasets*iteration)))\n",
    "\n",
    "        # calculate scaling factor\n",
    "        total_estimated_rewards = sum([math.exp(r*self.prev_eps) for r in self._estimated_reward.values()])\n",
    "        scaling_factor = (1-self.num_datasets*self.eps)/total_estimated_rewards\n",
    "\n",
    "        # update weights\n",
    "        for name in self.dataset_names:\n",
    "            self.weights[self.dataset_map[name]] = math.exp(self._estimated_reward[name]*self.prev_eps)*scaling_factor + self.eps\n",
    "\n",
    "        # update probabilities\n",
    "        total_weights = sum(self.weights)\n",
    "        for name in self.dataset_names:\n",
    "            self._probabilities[name] = self.weights[self.dataset_map[name]]/total_weights\n",
    "\n",
    "        return list(self._probabilities.values())\n",
    "\n",
    "    def group_update(self, dataset_names: List[str], rewards: List, iteration: int):\n",
    "        # calculate epsilons\n",
    "        self.prev_eps = self.eps\n",
    "        self.eps = min(1/self.num_datasets, math.sqrt(math.log(self.num_datasets)/(self.num_datasets*iteration)))\n",
    "\n",
    "        # update cumulative estimated reward\n",
    "        for name, reward in zip(dataset_names, rewards):\n",
    "            # smoothed mean\n",
    "            # self._estimated_reward[name] = self.smoothing_factor*self._estimated_reward[name] + (1-self.smoothing_factor)*reward\n",
    "            # smoothed exponentiated mean\n",
    "            self._estimated_reward[name] = self.smoothing_factor*self._estimated_reward[name] + (1-self.smoothing_factor)*math.exp(reward)\n",
    "        # print(f\"Rank: {torch.distributed.get_rank()} -- estimated_reward {self._estimated_reward}\")\n",
    "\n",
    "        # calculate normalized scaling factor\n",
    "        total_estimated_rewards = sum((r*self.prev_eps) for r in self._estimated_reward.values())\n",
    "        scaling_factor = (1-self.num_datasets*self.eps)/total_estimated_rewards\n",
    "\n",
    "        # update weights\n",
    "        for name in self.dataset_names:\n",
    "            # self.weights[self.dataset_map[name]] = math.exp(self._estimated_reward[name]*self.prev_eps)*scaling_factor + self.eps\n",
    "            self.weights[self.dataset_map[name]] = self._estimated_reward[name]*self.prev_eps*scaling_factor + self.eps\n",
    "\n",
    "        # update probabilities\n",
    "        total_weights = sum(self.weights)\n",
    "        for name in self.dataset_names:\n",
    "            self._probabilities[name] = self.weights[self.dataset_map[name]]/total_weights\n",
    "\n",
    "        return list(self._probabilities.values())"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T14:31:00.263318Z",
     "start_time": "2025-03-13T14:31:00.259296Z"
    }
   },
   "cell_type": "code",
   "source": [
    "weights = SmoothedMeanWeightUpdater([\"a\", \"b\"], [0.5, 0.5])\n",
    "weights.update(\"a\", 4, 1)\n",
    "weights.update(\"b\", 4, 2)\n",
    "weights.update(\"a\", 2, 3)\n",
    "weights.update(\"a\", 0, 3)"
   ],
   "id": "eb303151b979bb20",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4925946534736098, 0.5074053465263902]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T16:06:00.265720Z",
     "start_time": "2025-03-13T16:05:55.779316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from numpy import dtype\n",
    "from pandas import DataFrame\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "model_path=\"base_models/granite-3.2-2b-instruct\"\n",
    "device= \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path\n",
    ")"
   ],
   "id": "dcc852072d46b42b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1af98faed72d458796ad7c877522550e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T16:06:01.209670Z",
     "start_time": "2025-03-13T16:06:00.837325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "with open(\"data/coco.ml.txt\") as f:\n",
    "    ml = f.readlines()\n",
    "\n",
    "with open(\"data/coco.en.txt\") as f:\n",
    "    eng = f.readlines()\n",
    "\n",
    "def get_dataset(ml, eng):\n",
    "    ml = [sen.strip() for sen in ml]\n",
    "    eng = [sen.strip() for sen in eng]\n",
    "    return [{\"ml\": ml, \"eng\": eng, \"content\": f'Translate to english:<|end_of_text|>{ml}<|end_of_text|>{eng}<|end_of_text|>'} for ml, eng in zip(ml, eng)]\n",
    "\n",
    "dataset = get_dataset(ml, eng)\n",
    "n = 100\n",
    "train_dataset = Dataset.from_list(dataset[:n // 10 * 8])\n",
    "valid_dataset = Dataset.from_list(dataset[n // 10 * 8:n])\n",
    "dataset = DatasetDict({\"train\": train_dataset, \"validation\": valid_dataset})\n",
    "dataset"
   ],
   "id": "a1d2c6cdc5597efc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ml', 'eng', 'content'],\n",
       "        num_rows: 80\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['ml', 'eng', 'content'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T16:06:05.206837Z",
     "start_time": "2025-03-13T16:06:05.112591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "args = TrainingArguments(\"ml_to_en\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"content\"], truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].clone()\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, remove_columns=dataset['train'].column_names, batched=True)\n",
    "tokenized_datasets"
   ],
   "id": "a6ad464e7809412d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d8c66ca82dc04c5495e5eb6c1fe489d2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bcff67ba843c4755b32463c049d11226"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 80\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-13T16:31:42.763818Z",
     "start_time": "2025-03-13T16:29:30.314981Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm.auto import tqdm\n",
    "from transformers import get_scheduler, DataCollatorWithPadding\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"train\"], shuffle=True, batch_size=1, collate_fn=data_collator\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    tokenized_datasets[\"validation\"], batch_size=1, collate_fn=data_collator\n",
    ")\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = 100\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "data_loaders = {\n",
    "    \"train\": train_dataloader,\n",
    "    \"eval\": eval_dataloader,\n",
    "}\n",
    "\n",
    "data_loader_iters = {k: iter(v) for k, v in data_loaders.items()}\n",
    "weights = SmoothedMeanWeightUpdater([\"train\", \"eval\"], [0.5, 0.5])\n",
    "\n",
    "model.train()\n",
    "for i in range(num_training_steps):\n",
    "    batch_name = random.choices([\"train\", \"eval\"], weights=weights.weights)[0]\n",
    "    print(batch_name)\n",
    "    try:\n",
    "        batch = next(data_loader_iters[batch_name]) \n",
    "    except StopIteration:\n",
    "        data_loader_iters[batch_name] = iter(data_loaders[batch_name])\n",
    "        batch = next(data_loader_iters[batch_name])\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    outputs = model(**batch)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    res = weights.update(batch_name, loss.item(), i + 1)\n",
    "    print(loss, res)\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    optimizer.zero_grad()\n",
    "    progress_bar.update(1)"
   ],
   "id": "b06650651ce05471",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  0%|          | 0/240 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "07defb7039584a4d9c760b0058b74657"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval\n",
      "tensor(0.7216, device='mps:0', grad_fn=<NllLossBackward0>) [0.5, 0.5]\n",
      "train\n",
      "tensor(6.5066, device='mps:0', grad_fn=<NllLossBackward0>) [0.5837226944211507, 0.4162773055788494]\n",
      "eval\n",
      "tensor(2.7237, device='mps:0', grad_fn=<NllLossBackward0>) [0.6601110032765516, 0.33988899672344836]\n",
      "eval\n",
      "tensor(1.1982, device='mps:0', grad_fn=<NllLossBackward0>) [0.7056474942696611, 0.294352505730339]\n",
      "train\n",
      "tensor(1.3106, device='mps:0', grad_fn=<NllLossBackward0>) [0.7367231006648045, 0.2632768993351955]\n",
      "eval\n",
      "tensor(0.7517, device='mps:0', grad_fn=<NllLossBackward0>) [0.7596620857223804, 0.24033791427761972]\n",
      "eval\n",
      "tensor(1.3359, device='mps:0', grad_fn=<NllLossBackward0>) [0.7774899908727757, 0.22251000912722438]\n",
      "train\n",
      "tensor(1.2670, device='mps:0', grad_fn=<NllLossBackward0>) [0.7918567751490906, 0.20814322485090947]\n",
      "train\n",
      "tensor(1.1806, device='mps:0', grad_fn=<NllLossBackward0>) [0.8037351917354268, 0.19626480826457324]\n",
      "train\n",
      "tensor(1.4122, device='mps:0', grad_fn=<NllLossBackward0>) [0.8137021161583401, 0.18629788384165993]\n",
      "train\n",
      "tensor(0.8356, device='mps:0', grad_fn=<NllLossBackward0>) [0.8220302062275328, 0.17796979377246727]\n",
      "train\n",
      "tensor(0.8827, device='mps:0', grad_fn=<NllLossBackward0>) [0.8287281035605676, 0.17127189643943236]\n",
      "train\n",
      "tensor(1.1703, device='mps:0', grad_fn=<NllLossBackward0>) [0.8335964641386056, 0.1664035358613944]\n",
      "train\n",
      "tensor(1.9055, device='mps:0', grad_fn=<NllLossBackward0>) [0.8365902249833361, 0.1634097750166639]\n",
      "train\n",
      "tensor(1.0427, device='mps:0', grad_fn=<NllLossBackward0>) [0.8366839831533467, 0.1633160168466533]\n",
      "train\n",
      "tensor(0.7960, device='mps:0', grad_fn=<NllLossBackward0>) [0.8334899106536102, 0.16651008934638983]\n",
      "train\n",
      "tensor(0.5317, device='mps:0', grad_fn=<NllLossBackward0>) [0.8265639464341509, 0.17343605356584912]\n",
      "train\n",
      "tensor(0.7870, device='mps:0', grad_fn=<NllLossBackward0>) [0.8162755364468284, 0.1837244635531716]\n",
      "train\n",
      "tensor(0.9137, device='mps:0', grad_fn=<NllLossBackward0>) [0.8030091538106474, 0.19699084618935253]\n",
      "train\n",
      "tensor(0.9977, device='mps:0', grad_fn=<NllLossBackward0>) [0.7873929433419856, 0.2126070566580144]\n",
      "train\n",
      "tensor(1.0395, device='mps:0', grad_fn=<NllLossBackward0>) [0.7701043027561857, 0.22989569724381426]\n",
      "train\n",
      "tensor(1.0390, device='mps:0', grad_fn=<NllLossBackward0>) [0.751751996254447, 0.24824800374555306]\n",
      "train\n",
      "tensor(1.1356, device='mps:0', grad_fn=<NllLossBackward0>) [0.7334153748545102, 0.26658462514548986]\n",
      "train\n",
      "tensor(0.6910, device='mps:0', grad_fn=<NllLossBackward0>) [0.7134787387592421, 0.28652126124075805]\n",
      "eval\n",
      "tensor(1.0911, device='mps:0', grad_fn=<NllLossBackward0>) [0.7098892703839113, 0.29011072961608875]\n",
      "train\n",
      "tensor(0.8652, device='mps:0', grad_fn=<NllLossBackward0>) [0.6911188769924949, 0.3088811230075051]\n",
      "eval\n",
      "tensor(0.7751, device='mps:0', grad_fn=<NllLossBackward0>) [0.6890981569504787, 0.3109018430495213]\n",
      "train\n",
      "tensor(0.9408, device='mps:0', grad_fn=<NllLossBackward0>) [0.6717495357182223, 0.3282504642817778]\n",
      "train\n",
      "tensor(0.9437, device='mps:0', grad_fn=<NllLossBackward0>) [0.6555752599252124, 0.34442474007478774]\n",
      "eval\n",
      "tensor(1.4184, device='mps:0', grad_fn=<NllLossBackward0>) [0.6503114324248369, 0.34968856757516315]\n",
      "train\n",
      "tensor(0.6732, device='mps:0', grad_fn=<NllLossBackward0>) [0.6343136269549926, 0.3656863730450074]\n",
      "eval\n",
      "tensor(1.4275, device='mps:0', grad_fn=<NllLossBackward0>) [0.6295066999229826, 0.3704933000770175]\n",
      "train\n",
      "tensor(1.0589, device='mps:0', grad_fn=<NllLossBackward0>) [0.6167245131451955, 0.38327548685480456]\n",
      "train\n",
      "tensor(1.0271, device='mps:0', grad_fn=<NllLossBackward0>) [0.6049629314741204, 0.3950370685258796]\n",
      "train\n",
      "tensor(0.6968, device='mps:0', grad_fn=<NllLossBackward0>) [0.5928389815030294, 0.4071610184969707]\n",
      "eval\n",
      "tensor(0.7917, device='mps:0', grad_fn=<NllLossBackward0>) [0.5925175784286765, 0.40748242157132347]\n",
      "train\n",
      "tensor(1.2232, device='mps:0', grad_fn=<NllLossBackward0>) [0.5843590111681815, 0.4156409888318186]\n",
      "train\n",
      "tensor(0.9560, device='mps:0', grad_fn=<NllLossBackward0>) [0.5755252834817549, 0.42447471651824514]\n",
      "train\n",
      "tensor(0.8230, device='mps:0', grad_fn=<NllLossBackward0>) [0.5669948703474801, 0.43300512965251997]\n",
      "eval\n",
      "tensor(0.8273, device='mps:0', grad_fn=<NllLossBackward0>) [0.5667654607172153, 0.43323453928278466]\n",
      "train\n",
      "tensor(0.7605, device='mps:0', grad_fn=<NllLossBackward0>) [0.5589559139275757, 0.4410440860724243]\n",
      "train\n",
      "tensor(0.9083, device='mps:0', grad_fn=<NllLossBackward0>) [0.5526046011215175, 0.4473953988784825]\n",
      "eval\n",
      "tensor(0.9593, device='mps:0', grad_fn=<NllLossBackward0>) [0.551901592919392, 0.448098407080608]\n",
      "eval\n",
      "tensor(1.1761, device='mps:0', grad_fn=<NllLossBackward0>) [0.5500890779110273, 0.44991092208897265]\n",
      "train\n",
      "tensor(0.8726, device='mps:0', grad_fn=<NllLossBackward0>) [0.5443843820225124, 0.45561561797748756]\n",
      "train\n",
      "tensor(0.8960, device='mps:0', grad_fn=<NllLossBackward0>) [0.539392892119472, 0.460607107880528]\n",
      "train\n",
      "tensor(0.7645, device='mps:0', grad_fn=<NllLossBackward0>) [0.534399397010348, 0.46560060298965206]\n",
      "train\n",
      "tensor(0.9045, device='mps:0', grad_fn=<NllLossBackward0>) [0.5305190917812951, 0.469480908218705]\n",
      "train\n",
      "tensor(0.9051, device='mps:0', grad_fn=<NllLossBackward0>) [0.5270605438273726, 0.47293945617262745]\n",
      "train\n",
      "tensor(0.4381, device='mps:0', grad_fn=<NllLossBackward0>) [0.522364404348859, 0.47763559565114105]\n",
      "eval\n",
      "tensor(0.8054, device='mps:0', grad_fn=<NllLossBackward0>) [0.5227736024927505, 0.4772263975072495]\n",
      "train\n",
      "tensor(1.1242, device='mps:0', grad_fn=<NllLossBackward0>) [0.5212552003792794, 0.4787447996207206]\n",
      "eval\n",
      "tensor(1.3517, device='mps:0', grad_fn=<NllLossBackward0>) [0.5188358849464108, 0.48116411505358936]\n",
      "train\n",
      "tensor(0.8983, device='mps:0', grad_fn=<NllLossBackward0>) [0.5164499257811458, 0.48355007421885426]\n",
      "train\n",
      "tensor(0.9752, device='mps:0', grad_fn=<NllLossBackward0>) [0.5146532930215936, 0.48534670697840643]\n",
      "train\n",
      "tensor(1.5236, device='mps:0', grad_fn=<NllLossBackward0>) [0.5162849725187315, 0.4837150274812686]\n",
      "train\n",
      "tensor(0.4983, device='mps:0', grad_fn=<NllLossBackward0>) [0.5128421127692546, 0.48715788723074543]\n",
      "train\n",
      "tensor(0.6008, device='mps:0', grad_fn=<NllLossBackward0>) [0.5100711444913879, 0.4899288555086121]\n",
      "train\n",
      "tensor(0.5282, device='mps:0', grad_fn=<NllLossBackward0>) [0.5073965634127494, 0.49260343658725053]\n",
      "train\n",
      "tensor(0.7039, device='mps:0', grad_fn=<NllLossBackward0>) [0.5055473756146732, 0.4944526243853267]\n",
      "eval\n",
      "tensor(1.4131, device='mps:0', grad_fn=<NllLossBackward0>) [0.5031975586896139, 0.49680244131038614]\n",
      "train\n",
      "tensor(1.6935, device='mps:0', grad_fn=<NllLossBackward0>) [0.5070549425446841, 0.49294505745531597]\n",
      "train\n",
      "tensor(0.7551, device='mps:0', grad_fn=<NllLossBackward0>) [0.5052065224881213, 0.49479347751187874]\n",
      "eval\n",
      "tensor(1.4023, device='mps:0', grad_fn=<NllLossBackward0>) [0.5032033192756425, 0.49679668072435756]\n",
      "eval\n",
      "tensor(1.2306, device='mps:0', grad_fn=<NllLossBackward0>) [0.5024290158738574, 0.4975709841261427]\n",
      "eval\n",
      "tensor(0.7259, device='mps:0', grad_fn=<NllLossBackward0>) [0.5038574195038326, 0.4961425804961675]\n",
      "eval\n",
      "tensor(1.1634, device='mps:0', grad_fn=<NllLossBackward0>) [0.5033652796041093, 0.49663472039589085]\n",
      "train\n",
      "tensor(0.9469, device='mps:0', grad_fn=<NllLossBackward0>) [0.5024677009529227, 0.4975322990470774]\n",
      "train\n",
      "tensor(0.7477, device='mps:0', grad_fn=<NllLossBackward0>) [0.5009552413164311, 0.4990447586835689]\n",
      "train\n",
      "tensor(0.7880, device='mps:0', grad_fn=<NllLossBackward0>) [0.4997424246376578, 0.5002575753623423]\n",
      "eval\n",
      "tensor(0.5137, device='mps:0', grad_fn=<NllLossBackward0>) [0.501647607962251, 0.498352392037749]\n",
      "train\n",
      "tensor(0.5254, device='mps:0', grad_fn=<NllLossBackward0>) [0.4998005255190626, 0.5001994744809375]\n",
      "train\n",
      "tensor(0.6069, device='mps:0', grad_fn=<NllLossBackward0>) [0.4983725852428604, 0.5016274147571397]\n",
      "eval\n",
      "tensor(0.6112, device='mps:0', grad_fn=<NllLossBackward0>) [0.4998108899540721, 0.500189110045928]\n",
      "train\n",
      "tensor(0.7230, device='mps:0', grad_fn=<NllLossBackward0>) [0.4988744137620214, 0.5011255862379786]\n",
      "train\n",
      "tensor(1.1590, device='mps:0', grad_fn=<NllLossBackward0>) [0.4996968842802938, 0.5003031157197063]\n",
      "train\n",
      "tensor(0.8347, device='mps:0', grad_fn=<NllLossBackward0>) [0.49913898739177126, 0.5008610126082288]\n",
      "eval\n",
      "tensor(0.5082, device='mps:0', grad_fn=<NllLossBackward0>) [0.5006632059076165, 0.4993367940923836]\n",
      "eval\n",
      "tensor(0.3243, device='mps:0', grad_fn=<NllLossBackward0>) [0.50242318161019, 0.49757681838981005]\n",
      "eval\n",
      "tensor(0.6625, device='mps:0', grad_fn=<NllLossBackward0>) [0.5031885048457057, 0.49681149515429435]\n",
      "eval\n",
      "tensor(0.3118, device='mps:0', grad_fn=<NllLossBackward0>) [0.504688799122503, 0.4953112008774971]\n",
      "eval\n",
      "tensor(0.4371, device='mps:0', grad_fn=<NllLossBackward0>) [0.505762942966141, 0.4942370570338591]\n",
      "train\n",
      "tensor(0.6343, device='mps:0', grad_fn=<NllLossBackward0>) [0.5046532711452927, 0.49534672885470726]\n",
      "train\n",
      "tensor(0.6078, device='mps:0', grad_fn=<NllLossBackward0>) [0.5035930799008602, 0.49640692009913984]\n",
      "eval\n",
      "tensor(0.7956, device='mps:0', grad_fn=<NllLossBackward0>) [0.5036135192903666, 0.4963864807096335]\n",
      "train\n",
      "tensor(0.6058, device='mps:0', grad_fn=<NllLossBackward0>) [0.5026670624692928, 0.4973329375307072]\n",
      "train\n",
      "tensor(0.7696, device='mps:0', grad_fn=<NllLossBackward0>) [0.5022749263444015, 0.49772507365559854]\n",
      "eval\n",
      "tensor(0.8398, device='mps:0', grad_fn=<NllLossBackward0>) [0.5021598582445213, 0.49784014175547875]\n",
      "train\n",
      "tensor(0.7173, device='mps:0', grad_fn=<NllLossBackward0>) [0.501660673486777, 0.49833932651322294]\n",
      "train\n",
      "tensor(0.9929, device='mps:0', grad_fn=<NllLossBackward0>) [0.5021031985324026, 0.49789680146759746]\n",
      "eval\n",
      "tensor(0.4469, device='mps:0', grad_fn=<NllLossBackward0>) [0.5030243815958969, 0.4969756184041032]\n",
      "train\n",
      "tensor(0.4443, device='mps:0', grad_fn=<NllLossBackward0>) [0.5018694348120446, 0.49813056518795534]\n",
      "train\n",
      "tensor(0.6755, device='mps:0', grad_fn=<NllLossBackward0>) [0.5013853775223847, 0.49861462247761534]\n",
      "eval\n",
      "tensor(0.4281, device='mps:0', grad_fn=<NllLossBackward0>) [0.5022442977806822, 0.4977557022193178]\n",
      "eval\n",
      "tensor(0.4882, device='mps:0', grad_fn=<NllLossBackward0>) [0.5028823945530807, 0.49711760544691935]\n",
      "eval\n",
      "tensor(0.6509, device='mps:0', grad_fn=<NllLossBackward0>) [0.5030678407643215, 0.4969321592356785]\n",
      "eval\n",
      "tensor(0.4411, device='mps:0', grad_fn=<NllLossBackward0>) [0.5037115862507603, 0.49628841374923977]\n",
      "eval\n",
      "tensor(0.5801, device='mps:0', grad_fn=<NllLossBackward0>) [0.5039791425251596, 0.4960208574748404]\n",
      "eval\n",
      "tensor(0.6943, device='mps:0', grad_fn=<NllLossBackward0>) [0.5039327185864149, 0.4960672814135851]\n",
      "eval\n",
      "tensor(0.6302, device='mps:0', grad_fn=<NllLossBackward0>) [0.5040518545929141, 0.4959481454070859]\n"
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
