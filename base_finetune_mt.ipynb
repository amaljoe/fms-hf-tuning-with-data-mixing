{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-08T17:23:44.503955Z",
     "start_time": "2025-03-08T17:23:39.523284Z"
    }
   },
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "model_path=\"base_models/granite-3.2-2b-instruct\"\n",
    "device= \"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_path\n",
    ")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a729e11c1234f65bc54aff3fdfc9fc5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T17:57:08.298284Z",
     "start_time": "2025-03-08T17:57:08.295226Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer(\"input\", \"input\", \"input\", \"inputs\", return_tensors=\"pt\")",
   "id": "fdec98e789dbb275",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[1131, 1131]]), 'attention_mask': tensor([[1, 1]]), 'labels': tensor([[1131, 8075]])}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "67a8c0b1b01bb92d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T18:35:18.934076Z",
     "start_time": "2025-03-08T18:35:18.625398Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "with open(\"data/coco.ml.txt\") as f:\n",
    "    ml = f.readlines()\n",
    "\n",
    "with open(\"data/coco.en.txt\") as f:\n",
    "    eng = f.readlines()\n",
    "\n",
    "def get_dataset(ml, eng):\n",
    "    ml = [sen.strip() for sen in ml]\n",
    "    eng = [sen.strip() for sen in eng]\n",
    "    return [{\"ml\": ml, \"eng\": eng, \"content\": f'Translate to english:<|end_of_text|>{ml}<|end_of_text|>{eng}<|end_of_text|>'} for ml, eng in zip(ml, eng)]\n",
    "\n",
    "n = 100\n",
    "dataset = get_dataset(ml, eng)\n",
    "train_dataset = Dataset.from_list(dataset[:n // 10 * 8])\n",
    "valid_dataset = Dataset.from_list(dataset[n // 10 * 8:n])\n",
    "dataset = DatasetDict({\"train\": train_dataset, \"validation\": valid_dataset})\n",
    "dataset"
   ],
   "id": "f508b0757560a4d6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ml', 'eng', 'content'],\n",
       "        num_rows: 80\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['ml', 'eng', 'content'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T18:44:59.125761Z",
     "start_time": "2025-03-08T18:44:58.988422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "args = TrainingArguments(\"ml_to_en\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"content\"], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].clone()\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ],
   "id": "30d28654e4927ad4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/80 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "914bce85f5ea44f7916e306416a39f6d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/20 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7acdef28472f4233be7a0652ba4423fe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['ml', 'eng', 'content', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 80\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['ml', 'eng', 'content', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 20\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-08T18:45:26.540104Z",
     "start_time": "2025-03-08T18:45:01.146580Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\")\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "id": "3f67fafc6fe2679d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/30 : < :, Epoch 0.10/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 45.47 GB, other allocations: 222.19 MB, max allowed: 45.90 GB). Tried to allocate 235.51 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[105], line 15\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Trainer\n\u001B[1;32m      7\u001B[0m trainer \u001B[38;5;241m=\u001B[39m Trainer(\n\u001B[1;32m      8\u001B[0m     model,\n\u001B[1;32m      9\u001B[0m     training_args,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     12\u001B[0m     tokenizer\u001B[38;5;241m=\u001B[39mtokenizer,\n\u001B[1;32m     13\u001B[0m )\n\u001B[0;32m---> 15\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/llms-general/lib/python3.11/site-packages/transformers/trainer.py:2052\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   2050\u001B[0m         hf_hub_utils\u001B[38;5;241m.\u001B[39menable_progress_bars()\n\u001B[1;32m   2051\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2052\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m inner_training_loop(\n\u001B[1;32m   2053\u001B[0m         args\u001B[38;5;241m=\u001B[39margs,\n\u001B[1;32m   2054\u001B[0m         resume_from_checkpoint\u001B[38;5;241m=\u001B[39mresume_from_checkpoint,\n\u001B[1;32m   2055\u001B[0m         trial\u001B[38;5;241m=\u001B[39mtrial,\n\u001B[1;32m   2056\u001B[0m         ignore_keys_for_eval\u001B[38;5;241m=\u001B[39mignore_keys_for_eval,\n\u001B[1;32m   2057\u001B[0m     )\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/llms-general/lib/python3.11/site-packages/transformers/trainer.py:2388\u001B[0m, in \u001B[0;36mTrainer._inner_training_loop\u001B[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[1;32m   2385\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallback_handler\u001B[38;5;241m.\u001B[39mon_step_begin(args, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol)\n\u001B[1;32m   2387\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39maccumulate(model):\n\u001B[0;32m-> 2388\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining_step(model, inputs)\n\u001B[1;32m   2390\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   2391\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   2392\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_xla_available()\n\u001B[1;32m   2393\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[1;32m   2394\u001B[0m ):\n\u001B[1;32m   2395\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   2396\u001B[0m     tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/llms-general/lib/python3.11/site-packages/transformers/trainer.py:3485\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(self, model, inputs)\u001B[0m\n\u001B[1;32m   3482\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb\u001B[38;5;241m.\u001B[39mreduce_mean()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m   3484\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss_context_manager():\n\u001B[0;32m-> 3485\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss(model, inputs)\n\u001B[1;32m   3487\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m inputs\n\u001B[1;32m   3488\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   3489\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mtorch_empty_cache_steps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   3490\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mtorch_empty_cache_steps \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m   3491\u001B[0m ):\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/llms-general/lib/python3.11/site-packages/transformers/trainer.py:3532\u001B[0m, in \u001B[0;36mTrainer.compute_loss\u001B[0;34m(self, model, inputs, return_outputs)\u001B[0m\n\u001B[1;32m   3530\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   3531\u001B[0m     labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 3532\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39minputs)\n\u001B[1;32m   3533\u001B[0m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[1;32m   3534\u001B[0m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[1;32m   3535\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpast_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/llms-general/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/llms-general/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m/opt/homebrew/anaconda3/envs/llms-general/lib/python3.11/site-packages/transformers/models/granite/modeling_granite.py:1108\u001B[0m, in \u001B[0;36mGraniteForCausalLM.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001B[0m\n\u001B[1;32m   1105\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1106\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1107\u001B[0m     \u001B[38;5;66;03m# Shift so that tokens < n predict n\u001B[39;00m\n\u001B[0;32m-> 1108\u001B[0m     shift_logits \u001B[38;5;241m=\u001B[39m logits[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, :\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, :]\u001B[38;5;241m.\u001B[39mcontiguous()\n\u001B[1;32m   1109\u001B[0m     shift_labels \u001B[38;5;241m=\u001B[39m labels[\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;241m.\u001B[39m, \u001B[38;5;241m1\u001B[39m:]\u001B[38;5;241m.\u001B[39mcontiguous()\n\u001B[1;32m   1110\u001B[0m     \u001B[38;5;66;03m# Flatten the tokens\u001B[39;00m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: MPS backend out of memory (MPS allocated: 45.47 GB, other allocations: 222.19 MB, max allowed: 45.90 GB). Tried to allocate 235.51 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "execution_count": 105
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
